{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416b5c3a",
   "metadata": {},
   "source": [
    "# Using Machine Learning Tools Assignment 1\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this assignment, you will apply some popular machine learning techniques to the problem of predicting bike rental demand. A data set has been provided containing records of bike rentals in Seoul, collected during 2017-18.\n",
    "\n",
    "## General instructions\n",
    "\n",
    "This assignment is divided into several tasks. Use the spaces provided in this notebook to answer the questions posed in each task. Note that some questions require writing a small amount of code and some require graphical results. It is your responsibility to make sure your responses are clearly labelled and your code has been fully executed (with the correct results displayed) before submission!\n",
    "\n",
    "**Do not** manually edit the data set file we have provided! For marking purposes, it's important that your code is written to run correctly on the original data file.\n",
    "\n",
    "When creating graphical output, label is clearly, with appropriate titles, xlabels and ylabels, as appropriate.\n",
    "\n",
    "Chapter 2 of the reference book is based on a similar workflow to this prac, so you may look there for some further background and ideas. You can also use any other general resources on the internet that are relevant although do not use ones which directly relate to these questions with this dataset (which would normally only be found in someone else's assignment answers). If you take a large portion of code or text from the internet then you should reference where this was taken from, but we do not expect any references for small pieces of code, such as from documentation, blogs or tutorials. Taking, and adapting, small portions of code is expected and is common practice when solving real problems.\n",
    "\n",
    "The following code imports some of the essential libraries that you will need. You should not need to modify it, but you are expected to import other libraries as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74910097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680c63f",
   "metadata": {
    "tags": []
   },
   "source": [
    "**STEP1:** Load the data set from the csv file (SeoulBikeData.csv) into a DataFrame, and summarise it with at least two appropriate pandas functions\n",
    "Download the data set from MyUni using the link provided on the assignment page. A paper that describes one related version of this dataset is: Sathishkumar V E, Jangwoo Park, and Yongyun Cho. 'Using data mining techniques for bike sharing demand prediction in metropolitan city.' Computer Communications, Vol.153, pp.353-366, March, 2020. Feel free to look at this if you want more information about the dataset.\n",
    "\n",
    "The data is stored in a CSV (comma separated variable) file and contains the following information \n",
    "\n",
    " - Date: year-month-day\n",
    " - Rented Bike Count: Count of bikes rented at each hour\n",
    " - Hour: Hour of the day\n",
    " - Temperature: Temperature in Celsius\n",
    " - Humidity: %\n",
    " - Windspeed: m/s\n",
    " - Visibility: 10m\n",
    " - Dew point temperature: Celsius\n",
    " - Solar radiation: MJ/m2\n",
    " - Rainfall: mm\n",
    " - Snowfall: cm\n",
    " - Seasons: Winter, Spring, Summer, Autumn\n",
    " - Holiday: Holiday/No holiday\n",
    " - Functional Day: NoFunc(Non Functional Hours), Fun(Functional hours)\n",
    "\n",
    "**Load the data set from the csv file into a DataFrame, and summarise it with at least two appropriate pandas functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2293507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# The following code is used by the autograder\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# make sure your variable that contains that data from this step is the one assigned to step1_data\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m step1_data \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'ellipsis' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "data = ...\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step1_data\n",
    "step1_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d69305",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**STEP2:** To get a feeling for the data it is a good idea to do some form of simple visualisation. Display a set of histograms for the features as they are right now, prior to any cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9fb6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e3869fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "**STEP3:** The \"Functioning Day\" feature records whether the bike rental was open for business on that day. For this assignment we are only interested in predicting demand on days when the business is open, so remove rows from the DataFrame where the business is closed. After doing this, delete the Functioning Day feature from the DataFrame and verify that this worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f7bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = ...\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step3_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6455b8b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "The goal is to predict bike rental demand using historical data. To achieve this, you will use regression techniques with \"Bike Rental Count\" as the target feature for this prediction, *but for this*, it is important that all other features in the data are numerical. \n",
    "\n",
    "**STEP4:** Two of the features in the data, \"Holiday\" and \"Season\", need to be converted to numerical format. Write code to convert the \"Holiday\" feature to 0 or 1 from its current format. For the \"Season\" feature, a better solution would be to add 4 new columns, labeled as \"Winter\", \"Spring\", \"Summer\", and \"Autumn\". Each of these columns should store a 0 or 1, depending on the corresponding season in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fe5b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "data = ...\n",
    "\n",
    "\n",
    "step4_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f918ceca",
   "metadata": {
    "tags": []
   },
   "source": [
    "**STEP5** It is known that bike rentals depend strongly on whether it's a weekday or a weekend. **Replace the Date feature with a Weekday feature that stores 0 or 1 depending on whether the date represents a weekend or weekday.**  To do this, use the function `date_is_weekday` below, which returns 1 if it is a weekday and 0 if it is a weekend.\n",
    "\n",
    "**Apply the function to the Date column in your DataFrame** (you can use `DataFrame.transform` to apply it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220848fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def date_is_weekday(datestring):\n",
    "    ### return 0 if weekend, 1 if weekday\n",
    "    dsplit = datestring.split('/')\n",
    "    wday = datetime.datetime(int(dsplit[2]),int(dsplit[1]),int(dsplit[0])).weekday()\n",
    "    return int(wday<=4)\n",
    "\n",
    "### Your code to apply the function here:\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step5_data\n",
    "step5_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0b6d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "**STEP6** Convert all the remaining data to numerical format, with any non-numerical entries set to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554dc174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = ...\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step6_data\n",
    "step6_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548cf03b",
   "metadata": {
    "tags": []
   },
   "source": [
    "**STEP7** Use graphical methods to display your data and identify problematic entries. Set any problematic values in the numerical data to `np.nan` and check that this has worked. Once this is done, specify a **sklearn *pipeline* that will perform imputation** to replace problematic entries (nan values) with an appropriate **median** value ***and* any other pre-processing** that you think should be used. Just specify the pipeline - do ***not*** run it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ae7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "# keep the variable name pipeline_step7 as you will use it in STEP9\n",
    "pipeline_step7 = ...\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step7_data\n",
    "step7_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b37c278",
   "metadata": {
    "tags": []
   },
   "source": [
    "**STEP8:** Generate a pre-processed version of the entire dataset by applying the pipeline defined in STEP7. Then, create separate scatter plots for each feature against the target variable \"Bike Rental Count\" to visualize the strength of the relationship. Additionally, calculate the correlation of each feature with the target using either the pandas function corr() or numpy corrcoef() and find the 3 attributes that are the most correlated with bike rentals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637c57c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data = ...\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "# top_3 should be an array of 3 strings ['attribute name', 'attribute name','attribute name']\n",
    "top_3 = ...\n",
    "print(top_3)\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains a list of the names of the top 3 attributes is assigned to step3_data\n",
    "step8_data = top_3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31293b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "**STEP9:** Divide the data into training and test sets using an appropriate splitting method such that 20% of the data is kept for testing. Create a pipeline that includes the linear regression model in addition to the pipeline defined in STEP7. Fit the pipeline to the training set and calculate the rmse of the fit to evaluate its performance. As a comparison, compute the rmse that would be obtained by predicting the mean value of bike rentals for all training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30459946",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77eeded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you can use make_pipline to create a new pipiline by adding a model at the end of pipeline_step7 or you can simply create a new pipeline. \n",
    "# whatever you end up doing, make sure it is called pipeline_step9\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "pipeline_step9 = ...\n",
    "\n",
    "y_train = ...\n",
    "X_train = ...\n",
    "\n",
    "y_test = ...\n",
    "X_test = ...\n",
    "\n",
    "\n",
    "# calculate the RMSE of the fit to the training data\n",
    "rmse_train = ...\n",
    "# calculate the RMSE of the baseline model (by predicting the mean value of bike rentals for all training examples)\n",
    "rmse_baseline = ...\n",
    "\n",
    "print(\"RMSE for training data:\", rmse_train)\n",
    "print(\"RMSE for baseline (predicting mean):\", rmse_baseline)\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step9_data = rmse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64da5af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The following code will show a visualisation of the fit for your linear regression.\n",
    "# I will use your pipeline_step9 to predict on 200 points from the training data\n",
    "subset_size = 200\n",
    "y_train_pred = pipeline_step9.predict(X_train[:subset_size])\n",
    "\n",
    "# Then I create a scatterplot of predicted vs actual values using your variables from the cell above\n",
    "ax = sns.scatterplot(x=y_train[:subset_size], y=y_train_pred)\n",
    "# A perfect solution would look like the red line\n",
    "sns.lineplot(x=y_train[:subset_size], y=y_train[:subset_size], color='red')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8893f5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "**STEP10:**  Fit a Kernel Ridge regression model (imported from sklearn.kernel_ridge) to the X_train data from STEP9. Build a new pipeline that includes the Kernel Ridge regression model in addition to the pipeline defined in STEP7, and fit it to the training data using default settings. Generate a scatter plot of the predicted values against the actual values for the training data, and calculate the RMSE of the fit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68555625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# you can use make_pipline to create a new pipiline by adding a model at the end of pipeline_step7 or you can simply create a new pipeline. \n",
    "# whatever you end up doing, make sure it is called pipeline_step10\n",
    "pipeline_step10 = ...\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "y_pred_train_KR = ...\n",
    "\n",
    "# calculate rmse for training data\n",
    "rmse_train_KR = ...\n",
    "print('Kernel Ridge model RMSE on training data:', rmse_train_KR)\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step10_data = rmse_train_KR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aeda0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use your pipeline_step10 to predict on 200 points from the training data\n",
    "subset_size = 200\n",
    "y_train_pred = pipeline_step10.predict(X_train[:subset_size])\n",
    "\n",
    "# Then create a scatterplot of predicted vs actual values using your variables from the cell above\n",
    "ax = sns.scatterplot(x=y_train[:subset_size], y=y_train_pred)\n",
    "sns.lineplot(x=y_train[:subset_size], y=y_train[:subset_size], color='red')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8c8f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "**STEP11:** fit a Support Vector Regression (from sklearn.svm import SVR). As you did for STEP10, create a new pipeline using the pipelinr from STEP7 and this model and fit it to your training data, using the default settings. Again, generate a scatter plot of the predicted values against the actual values for the training data, and calculate the RMSE of the fit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15981b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# you can use make_pipline to create a new pipiline by adding a model at the end of pipeline_step7 or you can simply create a new pipeline. \n",
    "# whatever you end up doing, make sure it is called pipeline_step11\n",
    "pipeline_step11 = ...\n",
    "\n",
    "# make predictions on the training data\n",
    "y_pred_train_SVR = ...\n",
    "\n",
    "# calculate rmse for training data\n",
    "rmse_train_SVR = ...\n",
    "print('Support Vector Regression model RMSE on training data:', rmse_train_SVR)\n",
    "\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step11_data = rmse_train_SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d0c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use your pipeline_step10 to predict on 200 points from the training data\n",
    "subset_size = 200\n",
    "y_train_pred = pipeline_step11.predict(X_train[:subset_size])\n",
    "\n",
    "# Then create a scatterplot of predicted vs actual values using your variables from the cell above\n",
    "ax = sns.scatterplot(x=y_train[:subset_size], y=y_train_pred)\n",
    "sns.lineplot(x=y_train[:subset_size], y=y_train[:subset_size], color='red')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d9c9ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "**STEP12:** Perform a 10 fold cross validation for each of the three model (LinearRegression,KernelRidge,SVR). This splits the training set (that we've used above) into 10 equal size subsets, and uses each in turn as the validation set while training a model with the other 9. You should therefore have 10 rmse values for each cross validation run. Find the mean and standard deviation of the rmse values obtained for each model for the validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20056713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you might need some or all of the following imports\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "#Linear Regression CV mean and std RMSE from the 10 folds:\n",
    "rmse_LR_mean = ...\n",
    "rmse_LR_std  = ...\n",
    "print('Linear Regression CV Scores:') \n",
    "print(f'Mean: {rmse_LR_mean:.2f}, Std: {rmse_LR_std:.2f}\\n')\n",
    "\n",
    "\n",
    "#Linear Regression CV mean and std:\n",
    "rmse_KR_mean = ...\n",
    "rmse_KR_std  = ...\n",
    "print('Kernel Ridge Regression CV Scores:') \n",
    "print(f'Mean: {rmse_KR_mean:.2f}, Std: {rmse_KR_std:.2f}\\n')\n",
    "\n",
    "\n",
    "#Linear Regression CV mean and std:\n",
    "rmse_SVR_mean = ...\n",
    "rmse_SVR_std  = ...\n",
    "print('Support Vector Regression CV Scores:') \n",
    "print(f'Mean: {rmse_SVR_mean:.2f}, Std: {rmse_SVR_std:.2f}\\n')\n",
    "\n",
    "\n",
    "# The following code is used by the autograder\n",
    "step12_data = [rmse_LR_mean,rmse_KR_mean,rmse_SVR_mean]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a6094",
   "metadata": {
    "tags": []
   },
   "source": [
    "**STEP13:** Both the Kernel Ridge Regression and Support Vector Regression have hyperparameters that can be adjusted to suit the problem. Use grid search to systematically compare the generalisation performance (rmse) obtained with different hyperparameter settings (still with 10-fold CV). Use the sklearn function GridSearchCV to do this.\n",
    "\n",
    "For KernelRidge, vary the hyperparameter alpha. (note, if you are using KernelRidge as the last step in a pipeline, alpha is refered to as kernelridge__alpha) \n",
    "\n",
    "For SVR, vary the hyperparameter C. (note, if you are using SVR as the last step in a pipeline, C is refered to as SVR__C)\n",
    "\n",
    "Find the hyperparameter setting for each medel.\n",
    "\n",
    "Finally, train and apply both models, with the best hyperparameter settings, to the test set and report the performance as rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786987a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Define the GridSearchCV objects for each model\n",
    "kr_cv = ...\n",
    "svr_cv = ...\n",
    "\n",
    "# Fit the GridSearchCV objects to the training data\n",
    "...\n",
    "...\n",
    "\n",
    "# Print the best hyperparameter setting for each model\n",
    "print(\"Best hyperparameter setting for Kernel Ridge Regression:\", kr_cv.best_params_)\n",
    "print(\"Best hyperparameter setting for Support Vector Regression:\", svr_cv.best_params_)\n",
    "\n",
    "# Create pipeline using the best hyperparameter\n",
    "pipeline_best_kr = ...\n",
    "pipeline_best_svr = ...\n",
    "\n",
    "# Train and apply the chosen model to the test set\n",
    "...\n",
    "kr_predictions = ...\n",
    "kr_rmse = ...\n",
    "\n",
    "...\n",
    "svr_predictions = ...\n",
    "svr_rmse = ...\n",
    "\n",
    "print(\"Kernel Ridge Regression RMSE on test set:\", kr_rmse)\n",
    "print(\"Support Vector Regression RMSE on test set:\", svr_rmse)\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step13_data = [kr_rmse , svr_rmse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d30c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use your pipeline_best_svr to predict on 200 points from the training data\n",
    "subset_size = 200\n",
    "y_train_pred = pipeline_best_svr.predict(X_train[:subset_size])\n",
    "\n",
    "# Then create a scatterplot of predicted vs actual values using your variables from the cell above\n",
    "ax = sns.scatterplot(x=y_train[:subset_size], y=y_train_pred)\n",
    "sns.lineplot(x=y_train[:subset_size], y=y_train[:subset_size], color='red')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d881d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "step1": {
     "name": "step1",
     "points": 2,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "step10": {
     "name": "step10",
     "points": 3,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "step11": {
     "name": "step11",
     "points": 4,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "step12": {
     "name": "step12",
     "points": 5,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "step13": {
     "name": "step13",
     "points": 5,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "step3": {
     "name": "step3",
     "points": 3,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "step4": {
     "name": "step4",
     "points": 3,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "step5": {
     "name": "step5",
     "points": 3,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "step6": {
     "name": "step6",
     "points": 3,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "step7": {
     "name": "step7",
     "points": 3,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "step8": {
     "name": "step8",
     "points": 3,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "step9": {
     "name": "step9",
     "points": 3,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}